# Generated by Django 5.0.3 on 2024-04-05 23:05

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('app', '0006_config_claude_api_key'),
    ]

    operations = [
        migrations.AddField(
            model_name='config',
            name='claude_temperature',
            field=models.FloatField(default=0, help_text='Amount of randomness injected into the response.\n\nDefaults to 1.0. Ranges from 0.0 to 1.0. Use temperature closer to 0.0 for analytical / multiple choice, and closer to 1.0 for creative and generative tasks.\n\nNote that even with temperature of 0.0, the results will not be fully deterministic.\n        ', verbose_name='temperature'),
        ),
        migrations.AlterField(
            model_name='config',
            name='chat_gpt_frequency_penalty',
            field=models.FloatField(default=0, help_text="Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing \n        frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n<br>\n <a target='_blank' href='https://platform.openai.com/docs/guides/text-generation/parameter-details'>\n more information about frequency and presence penalties.</a>\n", verbose_name='frequency_penalty'),
        ),
        migrations.AlterField(
            model_name='config',
            name='chat_gpt_presence_penalty',
            field=models.FloatField(default=0, help_text="\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far,\n increasing the model's likelihood to talk about new topics.\n<br>\n <a target='_blank' href='https://platform.openai.com/docs/guides/text-generation/parameter-details'>\n more information about frequency and presence penalties.</a>\n        ", verbose_name='presence_penalty'),
        ),
        migrations.AlterField(
            model_name='config',
            name='chat_gpt_temperature',
            field=models.FloatField(default=0, help_text='\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random,\n        while lower values like 0.2 will make it more focused and deterministic.\n<br>\nWe generally recommend altering this or top_p but not both.\n        ', verbose_name='temperature'),
        ),
        migrations.AlterField(
            model_name='config',
            name='chat_gpt_top_p',
            field=models.FloatField(default=1, help_text='An alternative to sampling with temperature, called nucleus sampling, where the model considers\n         the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top \n         10% probability mass are considered.\n         <br>\n         We generally recommend altering this or temperature but not both\n         ', verbose_name='top_p'),
        ),
    ]
